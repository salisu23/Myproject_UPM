{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "lr = 0.01\n",
    "n_folds = 5\n",
    "lstm_input_size = 1\n",
    "hidden_state_size = 30\n",
    "batch_size = 30\n",
    "num_sequence_layers = 2\n",
    "output_dim = 11\n",
    "num_time_steps = 4000\n",
    "rnn_type = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=11, num_layers=2, rnn_type='LSTM'):\n",
    "        super(Bi_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #Define the initial linear hidden layer\n",
    "        self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = eval('nn.' + rnn_type)(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Forward pass through initial hidden layer\n",
    "        linear_input = self.init_linear(input)\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both\n",
    "        # have shape (batch_size, num_layers, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(linear_input)\n",
    "\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ION_Dataset_Sequential(Dataset):\n",
    "    def __init__(self, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        y = self.output[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "class ION_Dataset_Sequential_test(Dataset):\n",
    "    def __init__(self, input):\n",
    "        self.input = input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "#test_df = pd.read_csv('/kaggle/input/data-no-drift/test_detrend.csv')\n",
    "X = train_df['signal'].values.reshape(-1, num_time_steps, 1)\n",
    "y = pd.get_dummies(train_df['open_channels']).values.reshape(-1, num_time_steps, output_dim)\n",
    "test_input = test_df[\"signal\"].values.reshape(-1, num_time_steps, 1)\n",
    "train_input_mean = X.mean()\n",
    "train_input_sigma = X.std()\n",
    "test_input = (test_input-train_input_mean)/train_input_sigma\n",
    "test_preds = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n",
    "test = ION_Dataset_Sequential_test(test_input)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fold 0\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 1)\n",
      "###### Loading\n",
      "Epoch 1/250 \t loss=0.3242 \t train_f1=0.0536 \t val_loss=0.2834 \t val_f1=0.0366 \t time=308.59s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17648\\190408555.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;31m#y_pred = model(x_batch.cuda()) # DBG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Iterate through folds\n",
    "\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "local_val_score = 0\n",
    "models = {}\n",
    "\n",
    "k=0 #initialize fold number\n",
    "for tr_idx, val_idx in kfold.split(X, y):\n",
    "    test_p = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n",
    "\n",
    "    print('starting fold', k)\n",
    "    k += 1\n",
    "\n",
    "    print(6*'#', 'splitting and reshaping the data')\n",
    "    train_input = X[tr_idx]\n",
    "    print(train_input.shape)\n",
    "    train_target = y[tr_idx]\n",
    "    val_input = X[val_idx]\n",
    "    val_target = y[val_idx]\n",
    "    train_input_mean = train_input.mean()\n",
    "    train_input_sigma = train_input.std()\n",
    "    val_input = (val_input-train_input_mean)/train_input_sigma\n",
    "    train_input = (train_input-train_input_mean)/train_input_sigma\n",
    "\n",
    "    print(6*'#', 'Loading')\n",
    "    train = ION_Dataset_Sequential(train_input, train_target)\n",
    "    valid = ION_Dataset_Sequential(val_input, val_target)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    #Build tensor data for torch\n",
    "    train_preds = np.zeros((int(train_input.shape[0] * train_input.shape[1])))\n",
    "    val_preds = np.zeros((int(val_input.shape[0] * val_input.shape[1])))\n",
    "    best_val_preds = np.zeros((int(val_input.shape[0] * val_input.shape[1])))\n",
    "    train_targets = np.zeros((int(train_input.shape[0] * train_input.shape[1])))\n",
    "    avg_losses_f = []\n",
    "    avg_val_losses_f = []\n",
    "\n",
    "    #Define loss function\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    #Build model, initialize weights and define optimizer\n",
    "    model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # Using Adam optimizer\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=150, factor=0.1, min_lr=1e-8)  # Using ReduceLROnPlateau schedule\n",
    "    temp_val_loss = 9999999999\n",
    "    reached_val_score = 0\n",
    "\n",
    "    #Iterate through epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        #Train\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "            y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch.cpu())\n",
    "            #y_pred = model(x_batch.cuda()) # DBG\n",
    "            loss = loss_fn(y_pred.cpu(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "            train_preds[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = pred.reshape((-1))\n",
    "            train_targets[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = y_batch.detach().cpu().numpy().argmax(axis=2).reshape((-1))\n",
    "            del y_pred, loss, x_batch, y_batch, pred\n",
    "\n",
    "        #Evaluate\n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "            y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "            y_pred = model(x_batch.cpu()).detach()\n",
    "            #y_pred = model(x_batch.cuda()).detach() DBG\n",
    "            avg_val_loss += loss_fn(y_pred.cpu(), y_batch).item() / len(valid_loader)\n",
    "            pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "            val_preds[i * batch_size * val_input.shape[1]:(i + 1) * batch_size * val_input.shape[1]] = pred.reshape((-1))\n",
    "            del y_pred, x_batch, y_batch, pred\n",
    "        if avg_val_loss < temp_val_loss:\n",
    "            temp_val_loss = avg_val_loss\n",
    "\n",
    "        #Calculate F1-score\n",
    "        train_score = f1_score(train_targets, train_preds, average='macro')\n",
    "        val_score = f1_score(val_target.argmax(axis=2).reshape((-1)), val_preds, average='macro')\n",
    "\n",
    "        #Print output of epoch\n",
    "        elapsed_time = time.time() - start_time\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if epoch%10 == 0:\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t train_f1={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, avg_loss, train_score, avg_val_loss, val_score, elapsed_time))\n",
    "\n",
    "        if val_score > reached_val_score:\n",
    "            reached_val_score = val_score\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_val_preds = copy.deepcopy(val_preds)\n",
    "\n",
    "    #Calculate F1-score of the fold\n",
    "    val_score_fold = f1_score(val_target.argmax(axis=2).reshape((-1)), best_val_preds, average='macro')\n",
    "\n",
    "    #Save the fold's model in a dictionary\n",
    "    models[k] = best_model\n",
    "\n",
    "    #Print F1-score of the fold\n",
    "    print(\"BEST VALIDATION SCORE (F1): \", val_score_fold)\n",
    "    local_val_score += (1/n_folds) * val_score_fold\n",
    "\n",
    "#Print final average k-fold CV F1-score\n",
    "print(\"Final Score \", local_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(n_folds):\n",
    "    test_p = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n",
    "    k += 1\n",
    "\n",
    "    #Import model of fold k\n",
    "    model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(models[k])\n",
    "\n",
    "    #Make predictions on test data\n",
    "    model.eval()\n",
    "    for i, x_batch in enumerate(test_loader):\n",
    "        x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "        y_pred = model(x_batch.cpu()).detach()\n",
    "        #y_pred = model(x_batch.cuda()).detach()\n",
    "        pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "        test_p[i * batch_size * test_input.shape[1]:(i + 1) * batch_size * test_input.shape[1]] = pred.reshape((-1))\n",
    "        del y_pred, x_batch, pred\n",
    "    test_preds += (1/n_folds) * test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#Path('submission.csv').touch()\n",
    "Path('submission_bilstm.csv').tourch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"sample_submission.csv\", dtype = {'time': str})\n",
    "df_sub.open_channels = np.array(test_preds, np.int)\n",
    "df_sub.to_csv(\"submission_bilstm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

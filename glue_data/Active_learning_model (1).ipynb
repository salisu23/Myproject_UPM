{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCoycb2Xql59"
      },
      "source": [
        "**ACTICTIVE LEARNING USING PYTHON LABRARIES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwkgUazp9Yg4"
      },
      "source": [
        "Active learning is a machine learning technique where the model selects the most informative examples to label from a pool of unlabeled data. Here's an example of how to implement active learning for text data annotation in Python:\n",
        "\n",
        "1. Load your dataset into Python, split it into labeled and unlabeled data.\n",
        "\n",
        "2. Choose a text classification model and train it on the labeled data.\n",
        "3. Use the trained model to make predictions on the unlabeled data.\n",
        "4. Select the most informative examples from the unlabeled data using an uncertainty-based or diversity-based sampling method. Uncertainty-based sampling methods select examples where the model is most uncertain about the predicted label. Diversity-based sampling methods select examples that are the most dissimilar from the ones already labeled.\n",
        "4. Manually label the selected examples.\n",
        "6. Add the newly labeled data to the labeled data.\n",
        "7. Retrain the text classification model with the updated labeled data.\n",
        "8. Repeat steps 3-7 until the desired performance is achieved or the labeled data becomes too costly to obtain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elFbMrfv-g2q"
      },
      "source": [
        "Scikit-learn is a popular Python package for machine learning, and it provides various tools and functions for implementing active learning for text data annotation. Here's an example of how to implement active learning for text data annotation using Scikit-learn:\n",
        "\n",
        "1. Load your dataset into Python, split it into labeled and unlabeled data.\n",
        "2. Choose a text classification model, such as Support Vector Machines (SVM), and train it on the labeled data using Scikit-learn's SVM implementation.\n",
        "3. Use the trained model to make predictions on the unlabeled data using the predict() method.\n",
        "4. Select the most informative examples from the unlabeled data using an uncertainty-based or diversity-based sampling method. For example, you can use the uncertainty sampling method from the ActiveLearningClassifier class in Scikit-learn's semi-supervised learning module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv4CVzPn9A2R"
      },
      "outputs": [],
      "source": [
        "from sklearn.semi_supervised import ActiveLearningClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize an SVM classifier\n",
        "svm = SVC(kernel='linear', probability=True)\n",
        "\n",
        "# Initialize an ActiveLearningClassifier with the SVM classifier and uncertainty sampling method\n",
        "model = ActiveLearningClassifier(base_estimator=svm, query_strategy='uncertainty')\n",
        "\n",
        "# Train the model on the labeled data\n",
        "model.fit(X_labeled, y_labeled)\n",
        "\n",
        "# Use the model to make predictions on the unlabeled data\n",
        "y_unlabeled_pred = model.predict(X_unlabeled)\n",
        "\n",
        "# Get the uncertainty score for each example in the unlabeled data\n",
        "uncertainty_scores = model.predict_proba(X_unlabeled).max(axis=1)\n",
        "\n",
        "# Select the most uncertain examples from the unlabeled data\n",
        "num_samples_to_label = 10\n",
        "most_uncertain_indices = uncertainty_scores.argsort()[:num_samples_to_label]\n",
        "\n",
        "# Manually label the selected examples and add them to the labeled data\n",
        "X_labeled = np.vstack((X_labeled, X_unlabeled[most_uncertain_indices]))\n",
        "y_labeled = np.hstack((y_labeled, y_unlabeled[most_uncertain_indices]))\n",
        "\n",
        "# Retrain the model with the updated labeled data\n",
        "model.fit(X_labeled, y_labeled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-WiVUYF-pxp"
      },
      "source": [
        "5. Manually label the selected examples.\n",
        "6. Add the newly labeled data to the labeled data.\n",
        "7. Retrain the text classification model with the updated labeled data.\n",
        "8. Repeat steps 3-7 until the desired performance is achieved or the labeled data becomes too costly to obtain.\n",
        "Note that this is just a simple example of how to implement active learning using Scikit-learn. The specific details of the implementation will depend on your particular dataset and text classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7nTPG2G-8Ne"
      },
      "source": [
        "Active Learning Toolbox (ALTB) is a Python package that provides various tools and functions for implementing active learning. Here's an example of how to implement active learning for text data annotation using ALTB:\n",
        "\n",
        "1. Load your dataset into Python, split it into labeled and unlabeled data.\n",
        "2. Choose a text classification model, such as Support Vector Machines (SVM), and train it on the labeled data using ALTB's SVM implementation.\n",
        "3. Use the trained model to make predictions on the unlabeled data using the predict() method.\n",
        "4. Select the most informative examples from the unlabeled data using an uncertainty-based or diversity-based sampling method. For example, you can use the uncertainty sampling method from ALTB's query_by_committee module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU8w6QcE_H34"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from modAL.models import ActiveLearner\n",
        "from modAL.uncertainty import uncertainty_sampling\n",
        "\n",
        "# Initialize an SVM classifier\n",
        "svm = SVC(kernel='linear', probability=True)\n",
        "\n",
        "# Initialize an ActiveLearner with the SVM classifier and uncertainty sampling method\n",
        "learner = ActiveLearner(estimator=svm, query_strategy=uncertainty_sampling)\n",
        "\n",
        "# Train the model on the labeled data\n",
        "learner.fit(X_labeled, y_labeled)\n",
        "\n",
        "# Use the model to make predictions on the unlabeled data\n",
        "y_unlabeled_pred = learner.predict(X_unlabeled)\n",
        "\n",
        "# Get the uncertainty score for each example in the unlabeled data\n",
        "uncertainty_scores = learner.predict_proba(X_unlabeled).max(axis=1)\n",
        "\n",
        "# Select the most uncertain examples from the unlabeled data\n",
        "num_samples_to_label = 10\n",
        "most_uncertain_indices = uncertainty_scores.argsort()[:num_samples_to_label]\n",
        "\n",
        "# Manually label the selected examples and add them to the labeled data\n",
        "X_labeled = np.vstack((X_labeled, X_unlabeled[most_uncertain_indices]))\n",
        "y_labeled = np.hstack((y_labeled, y_unlabeled[most_uncertain_indices]))\n",
        "\n",
        "# Retrain the model with the updated labeled data\n",
        "learner.teach(X_labeled, y_labeled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT87SvGA_Ja3"
      },
      "source": [
        "5. Manually label the selected examples.\n",
        "6. Add the newly labeled data to the labeled data.\n",
        "7. Retrain the text classification model with the updated labeled data.\n",
        "8. Repeat steps 3-7 until the desired performance is achieved or the labeled data becomes too costly to obtain.\n",
        "Note that this is just a simple example of how to implement active learning using ALTB. The specific details of the implementation will depend on your particular dataset and text classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uupRhr51_TpW"
      },
      "source": [
        "modAL is another Python package that provides various tools and functions for implementing active learning. Here's an example of how to implement active learning for text data annotation using modAL:\n",
        "\n",
        "1. Load your dataset into Python, split it into labeled and unlabeled data.\n",
        "2. Choose a text classification model, such as Support Vector Machines (SVM), and train it on the labeled data using modAL's SVM implementation.\n",
        "3. Use the trained model to make predictions on the unlabeled data using the predict() method.\n",
        "4. Select the most informative examples from the unlabeled data using an uncertainty-based or diversity-based sampling method. For example, you can use the uncertainty sampling method from modAL's models module.\n",
        "Here's an example of how to perform uncertainty sampling using modAL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5b5eRw9_hu2"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from modAL.models import ActiveLearner\n",
        "from modAL.uncertainty import uncertainty_sampling\n",
        "\n",
        "# Initialize an SVM classifier\n",
        "svm = SVC(kernel='linear', probability=True)\n",
        "\n",
        "# Initialize an ActiveLearner with the SVM classifier and uncertainty sampling method\n",
        "learner = ActiveLearner(estimator=svm, X_training=X_labeled, y_training=y_labeled, query_strategy=uncertainty_sampling)\n",
        "\n",
        "# Use the model to make predictions on the unlabeled data\n",
        "y_unlabeled_pred = learner.predict(X_unlabeled)\n",
        "\n",
        "# Get the uncertainty score for each example in the unlabeled data\n",
        "uncertainty_scores = learner.predict_proba(X_unlabeled).max(axis=1)\n",
        "\n",
        "# Select the most uncertain examples from the unlabeled data\n",
        "num_samples_to_label = 10\n",
        "most_uncertain_indices = uncertainty_scores.argsort()[:num_samples_to_label]\n",
        "\n",
        "# Manually label the selected examples and add them to the labeled data\n",
        "X_labeled = np.vstack((X_labeled, X_unlabeled[most_uncertain_indices]))\n",
        "y_labeled = np.hstack((y_labeled, y_unlabeled[most_uncertain_indices]))\n",
        "\n",
        "# Retrain the model with the updated labeled data\n",
        "learner.teach(X_labeled=X_labeled, y_labeled=y_labeled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkxtz9BA_mde"
      },
      "source": [
        "5. Manually label the selected examples.\n",
        "6. Add the newly labeled data to the labeled data.\n",
        "7. Retrain the text classification model with the updated labeled data.\n",
        "8. Repeat steps 3-7 until the desired performance is achieved or the labeled data becomes too costly to obtain.\n",
        "Note that this is just a simple example of how to implement active learning using modAL. The specific details of the implementation will depend on your particular dataset and text classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6LE5ZR5p9Ms"
      },
      "source": [
        "**MULTI-TASK ACTIVE LEARNING WITH MT-DNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG7aMnnGoIyC"
      },
      "source": [
        "To implement a multi-task active learning model using the MT-DNN framework, you can follow these general steps:\n",
        "\n",
        "1. Choose the tasks you want to include in the multi-task learning model.\n",
        "Preprocess your data and split it into training, validation, and test sets for each task.\n",
        "2. Initialize the MT-DNN model with the appropriate number of layers, hidden units, and attention mechanisms.\n",
        "3. Train the model on the training set of each task, using a learning rate scheduler and early stopping.\n",
        "4. Evaluate the model on the validation set of each task to monitor its performance and avoid overfitting.\n",
        "5. Implement an active learning strategy, such as uncertainty sampling or query-by-committee, to select the most informative samples from the unlabeled data for each task.\n",
        "6. Label the selected samples and add them to the training set of the corresponding task.\n",
        "7. Retrain the model on the updated training set and repeat steps 5-7 until the desired performance is achieved.\n",
        "8. Finally, evaluate the model on the test set of each task to report its overall performance.\n",
        "\n",
        "To implement a multi-task active learning model for text data using PyTorch, you can follow these steps:\n",
        "\n",
        "1. Load and preprocess your text data. This might involve tokenizing the text, converting it to numerical features, and splitting it into training, validation, and test sets.\n",
        "\n",
        "2. efine the architecture of your MT-DNN model using PyTorch. This involves specifying the layers, activation functions, and attention mechanisms for each task. You can use pre-trained word embeddings such as BERT or GloVe as inputs to your model.\n",
        "\n",
        "3. Implement an active learning strategy such as uncertainty sampling or query-by-committee to select the most informative samples from the unlabeled data for each task. You can use a separate model or ensemble of models for this task.\n",
        "4. Train the MT-DNN model on the labeled training data for each task, using a learning rate scheduler and early stopping to prevent overfitting. You can use a joint training approach, where the model is trained on all tasks simultaneously, or a task-specific training approach, where the model is trained on each task separately.\n",
        "\n",
        "5. Evaluate the MT-DNN model on the validation set of each task to monitor its performance and avoid overfitting. You can use metrics such as accuracy, F1 score, or cross-entropy loss to evaluate the model.\n",
        "\n",
        "6. Label the selected samples from the unlabeled data and add them to the training set of the corresponding task.\n",
        "\n",
        "7. Retrain the MT-DNN model on the updated training set and repeat steps 5-7 until the desired performance is achieved.\n",
        "\n",
        "8. Finally, evaluate the MT-DNN model on the test set of each task to report its overall performance.\n",
        "\n",
        "You can use PyTorch's built-in modules for data loading, model training, and evaluation to simplify the implementation process. Additionally, there are several open-source PyTorch libraries such as Hugging Face Transformers and AllenNLP that provide pre-trained models and tools for implementing multi-task learning models for text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UNEF-c5pI0E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Load and preprocess the data\n",
        "train_data = ...\n",
        "val_data = ...\n",
        "test_data = ...\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_inputs = [tokenizer.encode(text, add_special_tokens=True) for text in train_data['text']]\n",
        "val_inputs = [tokenizer.encode(text, add_special_tokens=True) for text in val_data['text']]\n",
        "test_inputs = [tokenizer.encode(text, add_special_tokens=True) for text in test_data['text']]\n",
        "train_labels = train_data['label']\n",
        "val_labels = val_data['label']\n",
        "test_labels = test_data['label']\n",
        "train_dataset = TensorDataset(torch.tensor(train_inputs), torch.tensor(train_labels))\n",
        "val_dataset = TensorDataset(torch.tensor(val_inputs), torch.tensor(val_labels))\n",
        "test_dataset = TensorDataset(torch.tensor(test_inputs), torch.tensor(test_labels))\n",
        "\n",
        "# Define the architecture of the MT-DNN model\n",
        "class MT_DNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MT_DNN, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.task1 = nn.Linear(768, num_classes[0])\n",
        "        self.task2 = nn.Linear(768, num_classes[1])\n",
        "        self.task3 = nn.Linear(768, num_classes[2])\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        bert_output = self.bert(input_ids)[1]\n",
        "        task1_output = self.task1(bert_output)\n",
        "        task2_output = self.task2(bert_output)\n",
        "        task3_output = self.task3(bert_output)\n",
        "        return task1_output, task2_output, task3_output\n",
        "\n",
        "# Implement an active learning strategy to select samples from the unlabeled data\n",
        "def uncertainty_sampling(model, unlabeled_data, batch_size):\n",
        "    # Compute the uncertainty score for each sample in the unlabeled data\n",
        "    scores = ...\n",
        "    # Select the top k samples with the highest uncertainty scores\n",
        "    indices = ...\n",
        "    # Return the selected samples\n",
        "    return indices\n",
        "\n",
        "# Train the MT-DNN model on the labeled data\n",
        "model = MT_DNN([num_classes_task1, num_classes_task2, num_classes_task3])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "for epoch in range(num_epochs):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    for input_ids, labels in train_loader:\n",
        "        task1_labels = labels[:, 0]\n",
        "        task2_labels = labels[:, 1]\n",
        "        task3_labels = labels[:, 2]\n",
        "        task1_output, task2_output, task3_output = model(input_ids)\n",
        "        task1_loss = nn.CrossEntropyLoss()(task1_output, task1_labels)\n",
        "        task2_loss = nn.CrossEntropyLoss()(task2_output, task2_labels)\n",
        "        task3_loss = nn.CrossEntropyLoss()(task3_output, task3_labels)\n",
        "        loss = task1_loss + task2_loss + task3_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model on the validation data\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    task1_accuracy = 0.0\n",
        "    task2_accuracy = 0.0\n",
        "    task3_accuracy = 0.0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in val_loader:\n",
        "            task1_labels =\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2slJ_ixpNwS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Load and preprocess the data\n",
        "train_data = ...\n",
        "val_data = ...\n",
        "test_data = ...\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_inputs = [tokenizer.encode(text, add_special_tokens=True) for text in train_data['text']]\n",
        "val_inputs = [tokenizer.encode(text, add_special_tokens=True) for text in val_data['text']]\n",
        "test_inputs = [tokenizer.encode(text, add_special_tokens=True) for text in test_data['text']]\n",
        "train_labels = train_data['label']\n",
        "val_labels = val_data['label']\n",
        "test_labels = test_data['label']\n",
        "train_dataset = TensorDataset(torch.tensor(train_inputs), torch.tensor(train_labels))\n",
        "val_dataset = TensorDataset(torch.tensor(val_inputs), torch.tensor(val_labels))\n",
        "test_dataset = TensorDataset(torch.tensor(test_inputs), torch.tensor(test_labels))\n",
        "\n",
        "# Define the architecture of the MT-DNN model\n",
        "class MT_DNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MT_DNN, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.task1 = nn.Linear(768, num_classes[0])\n",
        "        self.task2 = nn.Linear(768, num_classes[1])\n",
        "        self.task3 = nn.Linear(768, num_classes[2])\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        bert_output = self.bert(input_ids)[1]\n",
        "        task1_output = self.task1(bert_output)\n",
        "        task2_output = self.task2(bert_output)\n",
        "        task3_output = self.task3(bert_output)\n",
        "        return task1_output, task2_output, task3_output\n",
        "\n",
        "# Implement an active learning strategy to select samples from the unlabeled data\n",
        "def uncertainty_sampling(model, unlabeled_data, batch_size):\n",
        "    # Compute the uncertainty score for each sample in the unlabeled data\n",
        "    scores = ...\n",
        "    # Select the top k samples with the highest uncertainty scores\n",
        "    indices = ...\n",
        "    # Return the selected samples\n",
        "    return indices\n",
        "\n",
        "# Train the MT-DNN model on the labeled data\n",
        "model = MT_DNN([num_classes_task1, num_classes_task2, num_classes_task3])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "for epoch in range(num_epochs):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    for input_ids, labels in train_loader:\n",
        "        task1_labels = labels[:, 0]\n",
        "        task2_labels = labels[:, 1]\n",
        "        task3_labels = labels[:, 2]\n",
        "        task1_output, task2_output, task3_output = model(input_ids)\n",
        "        task1_loss = nn.CrossEntropyLoss()(task1_output, task1_labels)\n",
        "        task2_loss = nn.CrossEntropyLoss()(task2_output, task2_labels)\n",
        "        task3_loss = nn.CrossEntropyLoss()(task3_output, task3_labels)\n",
        "        loss = task1_loss + task2_loss + task3_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model on the validation data\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    task1_accuracy = 0.0\n",
        "    task2_accuracy = 0.0\n",
        "    task3_accuracy = 0.0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in val_loader:\n",
        "            task1_labels =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joezpDZupc0q"
      },
      "source": [
        "To evaluate the MT-DNN model on the validation data, you can use a similar process as training. First, you need to load the validation data into a PyTorch DataLoader, which will split the data into batches. Then you can loop over each batch and compute the model's output for each task, and compare it with the corresponding ground truth labels to compute the accuracy for each task. Finally, you can aggregate the accuracies over all batches to compute the overall accuracy.\n",
        "\n",
        "Here's a sample code snippet for evaluating the MT-DNN model on the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9ZXiihFpfgR"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the validation data\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "task1_accuracy = 0.0\n",
        "task2_accuracy = 0.0\n",
        "task3_accuracy = 0.0\n",
        "with torch.no_grad():\n",
        "    for input_ids, labels in val_loader:\n",
        "        task1_labels = labels[:, 0]\n",
        "        task2_labels = labels[:, 1]\n",
        "        task3_labels = labels[:, 2]\n",
        "        task1_output, task2_output, task3_output = model(input_ids)\n",
        "        task1_predictions = torch.argmax(task1_output, dim=1)\n",
        "        task2_predictions = torch.argmax(task2_output, dim=1)\n",
        "        task3_predictions = torch.argmax(task3_output, dim=1)\n",
        "        task1_accuracy += torch.sum(task1_predictions == task1_labels).item()\n",
        "        task2_accuracy += torch.sum(task2_predictions == task2_labels).item()\n",
        "        task3_accuracy += torch.sum(task3_predictions == task3_labels).item()\n",
        "\n",
        "total_accuracy = (task1_accuracy + task2_accuracy + task3_accuracy) / (len(val_dataset) * 3)\n",
        "print('Validation accuracy: {:.2f}%'.format(total_accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDswT02Ppn_y"
      },
      "source": [
        "In this code, task1_accuracy, task2_accuracy, and task3_accuracy are accumulators that keep track of the number of correctly classified samples for each task. We loop over each batch of the validation data using the val_loader, and compute the model's output for each task using the model object. Then we compute the predictions for each task by taking the index of the highest output value using torch.argmax(). Finally, we add up the number of correct predictions for each task and compute the total accuracy over all tasks and samples in the validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYOCk1eSpstq"
      },
      "source": [
        "**DISTANT SUPERVISION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-sirQFuCPfW"
      },
      "source": [
        "Distant supervision is a technique for automatically labeling large amounts of data by leveraging existing knowledge bases or heuristics. Here's a high-level overview of how to implement a distant supervision model for text data annotation in Python:\n",
        "\n",
        "1. Choose a knowledge base or heuristic that is relevant to your task. For example, you might use a list of positive and negative words to label sentiment in text.\n",
        "\n",
        "2. Use the knowledge base or heuristic to automatically label a large amount of unlabeled text data.\n",
        "\n",
        "3. Preprocess the labeled data, such as by tokenizing the text and converting it to a numerical representation like bag-of-words or word embeddings.\n",
        "\n",
        "4. Split the labeled data into training and validation sets.\n",
        "\n",
        "5. Train a machine learning model on the labeled training data and evaluate it on the validation set.\n",
        "\n",
        "6. Optionally, use techniques like cross-validation or hyperparameter tuning to improve the performance of the model.\n",
        "\n",
        "7. Use the trained model to predict labels for new, unlabeled text data.\n",
        "\n",
        "Here's an example of how to implement a simple distant supervision model using the bag-of-words representation and logistic regression in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0gCsISvCdT2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the text data and label it using a knowledge base or heuristic\n",
        "# For this example, we'll use a list of positive and negative words\n",
        "pos_words = ['good', 'great', 'excellent', 'awesome']\n",
        "neg_words = ['bad', 'poor', 'terrible', 'awful']\n",
        "\n",
        "with open('text_data.txt', 'r') as f:\n",
        "    data = f.read().splitlines()\n",
        "\n",
        "X = np.array(data)\n",
        "y = np.zeros(len(X))\n",
        "for i, text in enumerate(X):\n",
        "    if any(word in text for word in pos_words):\n",
        "        y[i] = 1\n",
        "    elif any(word in text for word in neg_words):\n",
        "        y[i] = 0\n",
        "\n",
        "# Convert the text data to bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the labeled data into training and validation sets\n",
        "X_train, X_val = X[:800], X[800:]\n",
        "y_train, y_val = y[:800], y[800:]\n",
        "\n",
        "# Train a logistic regression model on the labeled training data\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the labeled validation data\n",
        "accuracy = model.score(X_val, y_val)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Use the trained model to predict labels for new, unlabeled text data\n",
        "X_unlabeled = vectorizer.transform(['This is a good movie.', 'This is a bad movie.'])\n",
        "predicted = model.predict(X_unlabeled)\n",
        "print(predicted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UunfF2p3CfGe"
      },
      "source": [
        "Note that this is just a simple example, and you may need to modify the code to suit your specific use case. For example, you might use a more sophisticated knowledge base or heuristic, such as a named entity recognition system or a regular expression pattern, to label the data. Additionally, you might use a more advanced machine learning model, such as a neural network or a support vector machine, to improve the performance of the system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78Hm_FyCCm8u"
      },
      "source": [
        "Using a Bi-LSTM (Bidirectional Long Short-Term Memory) model for distant supervision can potentially improve the performance compared to a simple logistic regression model. Here's an example of how to implement a Bi-LSTM model for text data annotation using distant supervision:\n",
        "\n",
        "Choose a knowledge base or heuristic that is relevant to your task. For example, you might use a list of positive and negative words to label sentiment in text.\n",
        "\n",
        "Use the knowledge base or heuristic to automatically label a large amount of unlabeled text data.\n",
        "\n",
        "Preprocess the labeled data, such as by tokenizing the text and converting it to a numerical representation like word embeddings.\n",
        "\n",
        "Split the labeled data into training and validation sets.\n",
        "\n",
        "Define the architecture of the Bi-LSTM model. The model should take as input the numerical representation of the text data and output a binary classification (e.g., positive or negative sentiment). The model can have multiple layers and include additional components like attention mechanisms, dropout, or batch normalization.\n",
        "\n",
        "Train the Bi-LSTM model on the labeled training data and evaluate it on the validation set.\n",
        "\n",
        "Optionally, use techniques like cross-validation or hyperparameter tuning to improve the performance of the model.\n",
        "\n",
        "Use the trained model to predict labels for new, unlabeled text data.\n",
        "\n",
        "Here's an example of how to implement a Bi-LSTM model for text data annotation using distant supervision in Python using the PyTorch library:\n",
        "\n",
        "\n",
        "\n",
        "# Use the trained model to predict labels for new, unlabeled text data\n",
        "# Preprocess the new text data and convert it to the same format as the training\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqQ7CH8ADWKu"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "# Define the Bi-LSTM model architecture\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size*2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = torch.mean(out, 1)\n",
        "        out = self.fc(out)\n",
        "        out = torch.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Set up the device (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the text data and label it using a knowledge base or heuristic\n",
        "# For this example, we'll use a list of positive and negative words\n",
        "pos_words = ['good', 'great', 'excellent', 'awesome']\n",
        "neg_words = ['bad', 'poor', 'terrible', 'awful']\n",
        "\n",
        "with open('text_data.txt', 'r') as f:\n",
        "    data = f.read().splitlines()\n",
        "\n",
        "X = np.array(data)\n",
        "y = np.zeros(len(X))\n",
        "for i, text in enumerate(X):\n",
        "    if any(word in text for word in pos_words):\n",
        "        y[i] = 1\n",
        "    elif any(word in text for word in neg_words):\n",
        "        y[i] =\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess the data\n",
        "# Tokenize the text and convert it to a numerical representation like word embeddings\n",
        "# For this example, we'll use pre-trained word embeddings\n",
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-wiki-gigaword-300\")\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize the text\n",
        "    tokens = text.lower().split()\n",
        "    # Convert each token to a word embedding\n",
        "    embeddings = [word_emb_model[token] for token in tokens if token in word_emb_model.vocab]\n",
        "    # Pad the sequence of embeddings to a fixed length\n",
        "    max_length = 50\n",
        "    if len(embeddings) < max_length:\n",
        "        embeddings += [np.zeros(300)] * (max_length - len(embeddings))\n",
        "    else:\n",
        "        embeddings = embeddings[:max_length]\n",
        "    return embeddings\n",
        "\n",
        "X = [preprocess(text) for text in X]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the dataset and data loader for the training and validation sets\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "val_dataset = TextDataset(X_val, y_val)\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Train the Bi-LSTM model\n",
        "input_size = 300\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "lr = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "model = BiLSTM(input_size, hidden_size, num_layers, dropout).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    val_acc = 0.0\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = torch.FloatTensor(inputs).to(device)\n",
        "        labels = torch.FloatTensor(labels).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += ((outputs > 0.5).int() == labels.unsqueeze(1)).sum().item()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = torch.FloatTensor(inputs).to(device)\n",
        "            labels = torch.FloatTensor(labels).to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\n",
        "            val_loss += loss.item()\n",
        "            val_acc += ((outputs > 0.5).int() == labels.unsqueeze(1)).sum().item()\n",
        "    train_loss /= len(train_loader)\n",
        "    val_loss /= len(val_loader)\n",
        "    train_acc /= len(train_dataset)\n",
        "    val_acc /= len(val_dataset)\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Train Acc: {:.4f}, Val Acc: {:.4f}'.format(epoch+1, num_epochs, train_loss, val_loss, train_acc, val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaiFS8EHDww4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocess the new text data and convert it to the same format as the training data\n",
        "new_texts = [\"This is a new document.\", \"Another new document here.\"]\n",
        "new_X = [preprocess(text) for text in new_texts]\n",
        "new_loader = DataLoader(new_X, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Use the trained model to predict labels for the new text data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs in new_loader:\n",
        "        inputs = torch.FloatTensor(inputs).to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions = (outputs > 0.5).int().tolist()\n",
        "        print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffsy_q2VEF4P"
      },
      "source": [
        "In this example, we preprocess the new text data using the preprocess() function, then convert it to the same format as the training data. We then create a data loader for the new data and use the trained model to make predictions for the new data using model.eval() and torch.no_grad(). Finally, we convert the predicted probabilities to binary labels using a threshold of 0.5 and print the predicted labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nz999BEqXZR"
      },
      "source": [
        "**INTERACTIVE DEEP LEARNING USING PYTORCH**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ1LWlPA_yDm"
      },
      "source": [
        "Implementing an interactive deep learning model for handling textual data in PyTorch involves several steps. Here's an example of how to implement such a model:\n",
        "\n",
        "1. Load your dataset into Python and preprocess the data (e.g., tokenize the text, convert it to a numerical representation).\n",
        "2. Define the architecture of the deep learning model. For this example, we'll 3. use a simple Convolutional Neural Network (CNN) with an interactive attention mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQnM1I8d_-7G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InteractiveCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, hidden_dim, num_classes):\n",
        "        super(InteractiveCNN, self).__init__()\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes\n",
        "        ])\n",
        "        \n",
        "        # Interactive attention layer\n",
        "        self.interactive_att = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, hidden_dim)\n",
        "        \n",
        "        # Output layer\n",
        "        self.out = nn.Linear(hidden_dim, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # batch_size x seq_len x embedding_dim\n",
        "        x = x.unsqueeze(1)  # batch_size x 1 x seq_len x embedding_dim\n",
        "        \n",
        "        # Apply convolutional layers\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            conv_output = conv(x)\n",
        "            relu_output = nn.functional.relu(conv_output)\n",
        "            pooled_output = nn.functional.max_pool2d(relu_output, (conv_output.size(2), 1))\n",
        "            conv_outputs.append(pooled_output)\n",
        "        x = torch.cat(conv_outputs, dim=1)  # batch_size x (num_filters * len(filter_sizes))\n",
        "        \n",
        "        # Apply fully connected layer\n",
        "        x = self.fc(x)  # batch_size x hidden_dim\n",
        "        \n",
        "        # Apply interactive attention\n",
        "        att_weights = nn.functional.softmax(self.interactive_att(x), dim=1)  # batch_size x hidden_dim\n",
        "        x = torch.sum(att_weights * x, dim=1)  # batch_size x hidden_dim\n",
        "        \n",
        "        # Apply output layer\n",
        "        x = self.out(x)  # batch_size x num_classes\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgG_mT90AESY"
      },
      "source": [
        "3. Split the data into training and testing sets.\n",
        "4. Define the loss function and optimizer for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRqGFMtlAMEP"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "model = InteractiveCNN(vocab_size, embedding_dim, num_filters, filter_sizes, hidden_dim, num_classes)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HPe3q-fAP0v"
      },
      "source": [
        "5. Train the model using the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QGjX5chAU5e"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        # Get the current batch of training data\n",
        "        batch_X = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "        \n",
        "        # Convert the training data to PyTorch tensors\n",
        "        batch_X = torch.LongTensor(batch_X)\n",
        "        batch_y = torch.LongTensor(batch_y)\n",
        "        \n",
        "        # Forward pass\n",
        "        y_pred = model(batch_X)\n",
        "        loss = loss_fn(y_pred, batch_y)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MHwVp2sAZMI"
      },
      "source": [
        "6. Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qafye7FkAgkm"
      },
      "outputs": [],
      "source": [
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Turn off gradient computation to speed up inference\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for i in range(0, len(X_test), batch_size):\n",
        "        # Get the current batch of test data\n",
        "        batch_X = X_test[i:i+batch_size]\n",
        "        batch_y = y_test[i:i+batch_size]\n",
        "        \n",
        "        # Convert the test data to PyTorch tensors\n",
        "        batch_X = torch.LongTensor(batch_X)\n",
        "        batch_y = torch.LongTensor(batch_y)\n",
        "        \n",
        "        # Forward pass\n",
        "        y_pred = model(batch_X)\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "        \n",
        "    print('Accuracy: {:.2f}%'.format(100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssubsQ7rAmp-"
      },
      "source": [
        "To enable human interaction for making corrections, you can implement an interface that allows users to view and correct the model's predictions for a given input text. Here's an example of how to implement such an interface using the PySimpleGUI library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k656nrMAsXm"
      },
      "outputs": [],
      "source": [
        "import PySimpleGUI as sg\n",
        "\n",
        "# Define the layout of the interface\n",
        "layout = [\n",
        "    [sg.Multiline(key='input', size=(80, 10), font=('Helvetica', 12))],\n",
        "    [sg.Text('Predicted Label:'), sg.Text(key='predicted_label', font=('Helvetica', 12))],\n",
        "    [sg.Text('Corrected Label:'), sg.InputText(key='corrected_label', font=('Helvetica', 12))],\n",
        "    [sg.Button('Submit', size=(10, 1))]\n",
        "]\n",
        "\n",
        "# Create the window\n",
        "window = sg.Window('Interactive Text Classification', layout)\n",
        "\n",
        "# Process input from the user\n",
        "while True:\n",
        "    event, values = window.read()\n",
        "    if event == sg.WIN_CLOSED:\n",
        "        break\n",
        "        \n",
        "    # Get the input text and make a prediction\n",
        "    input_text = values['input']\n",
        "    predicted_label = model.predict(input_text)\n",
        "    window['predicted_label'].update(predicted_label)\n",
        "    \n",
        "    # Wait for the user to correct the label\n",
        "    while True:\n",
        "        event, values = window.read()\n",
        "        if event == 'Submit':\n",
        "            corrected_label = values['corrected_label']\n",
        "            break\n",
        "    \n",
        "    # Update the model with the corrected label\n",
        "    model.update(input_text, corrected_label)\n",
        "\n",
        "# Close the window\n",
        "window.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKNEhuPcAxDG"
      },
      "source": [
        "In this example, the interface includes a text box for entering the input text, a label for displaying the predicted label, a text box for correcting the label, and a submit button. When the user enters input text and clicks the submit button, the model makes a prediction and displays the predicted label. The interface then waits for the user to correct the label by typing in the corrected label and clicking the submit button again. Once the corrected label is submitted, the model is updated with the corrected label, and the interface can be used to process the next input text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWpAgjykA3Vt"
      },
      "source": [
        "To combine all the code and create an end-to-end interactive text classification system, you can follow these steps:\n",
        "\n",
        "1. Import the necessary libraries, including PyTorch, Scikit-learn, Active Learning Toolbox, modAL, PySimpleGUI, and any other libraries you need.\n",
        "\n",
        "2. Load the labeled data and split it into training and testing sets. You can also split the training data into initial and unlabeled subsets for active learning.\n",
        "\n",
        "3. Train the initial model on the labeled training data.\n",
        "\n",
        "4. Use the initial model to predict the labels for the unlabeled data and use active learning to select the instances for annotation.\n",
        "\n",
        "5. Present the selected instances to a human annotator using an interface.\n",
        "\n",
        "6. After the human annotator corrects the labels, update the model with the corrected labels.\n",
        "\n",
        "7. Repeat steps 4-6 until a desired level of accuracy is achieved or the budget for annotation is exhausted.\n",
        "\n",
        "8. Evaluate the final model on the test data.\n",
        "\n",
        "9. Create an interface that allows users to enter input text and view the model's predictions.\n",
        "\n",
        "10. When the user corrects the model's prediction, update the model with the corrected label.\n",
        "\n",
        "11. Repeat steps 9-10 as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjH5qYi-BVSm"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from active_learning_toolbox import ActiveLearning\n",
        "from modAL.models import ActiveLearner\n",
        "import PySimpleGUI as sg\n",
        "\n",
        "# Load labeled data\n",
        "with open('labeled_data.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "X_labeled = [line.split('\\t')[0] for line in lines]\n",
        "y_labeled = [int(line.split('\\t')[1]) for line in lines]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split training data into initial and unlabeled subsets\n",
        "X_initial, X_unlabeled, y_initial, y_unlabeled = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert text to vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_initial = vectorizer.fit_transform(X_initial)\n",
        "X_unlabeled = vectorizer.transform(X_unlabeled)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "class TextClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.out = torch.nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.out(x)\n",
        "        x = torch.nn.functional.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "input_dim = X_initial.shape[1]\n",
        "hidden_dim = 100\n",
        "output_dim = len(set(y_labeled))\n",
        "model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Train the initial model\n",
        "X_initial = torch.FloatTensor(X_initial.toarray())\n",
        "y_initial = torch.LongTensor(y_initial)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_initial)\n",
        "    loss = criterion(outputs, y_initial)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Initialize the active learner\n",
        "learner = ActiveLearner(\n",
        "    estimator=model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYTjcoRoBjXj"
      },
      "outputs": [],
      "source": [
        "# Evaluate the final model on the test data\n",
        "X_test = torch.FloatTensor(X_test.toarray())\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Create a PySimpleGUI interface for presenting instances to the annotator\n",
        "sg.theme('LightGray1')\n",
        "layout = [\n",
        "    [sg.Multiline('', key='text', size=(70, 5), disabled=True)],\n",
        "    [sg.Radio('Negative', 'sentiment', key='neg'), sg.Radio('Positive', 'sentiment', key='pos')],\n",
        "    [sg.Button('Submit'), sg.Button('Exit')]\n",
        "]\n",
        "\n",
        "window = sg.Window('Text Classification', layout)\n",
        "\n",
        "# Present instances to the annotator using the interface\n",
        "while True:\n",
        "    index = learner.query(X_unlabeled)\n",
        "    instance = X_unlabeled[index]\n",
        "    text = vectorizer.inverse_transform(instance)[0][0]\n",
        "    window['text'].update(text)\n",
        "    event, values = window.read()\n",
        "    if event in (sg.WIN_CLOSED, 'Exit'):\n",
        "        break\n",
        "    if values['neg']:\n",
        "        label = 0\n",
        "    elif values['pos']:\n",
        "        label = 1\n",
        "    y_unlabeled[index] = label\n",
        "    X_labeled.append(X_unlabeled[index])\n",
        "    y_labeled.append(label)\n",
        "    X_unlabeled = np.delete(X_unlabeled, index, axis=0)\n",
        "    learner.teach(instance, label)\n",
        "\n",
        "# Update the model with the corrected labels\n",
        "X_labeled = torch.FloatTensor(vectorizer.transform(X_labeled).toarray())\n",
        "y_labeled = torch.LongTensor(y_labeled)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_labeled)\n",
        "    loss = criterion(outputs, y_labeled)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Create a PySimpleGUI interface for making predictions and correcting the model\n",
        "layout = [\n",
        "    [sg.Multiline('', key='input', size=(70, 5))],\n",
        "    [sg.Radio('Negative', 'sentiment', key='neg'), sg.Radio('Positive', 'sentiment', key='pos')],\n",
        "    [sg.Button('Submit'), sg.Button('Exit')]\n",
        "]\n",
        "\n",
        "window = sg.Window('Text Classification', layout)\n",
        "\n",
        "# Make predictions and correct the model using the interface\n",
        "while True:\n",
        "    event, values = window.read()\n",
        "    if event in (sg.WIN_CLOSED, 'Exit'):\n",
        "        break\n",
        "    input_text = values['input']\n",
        "    input_vector = vectorizer.transform([input_text]).toarray()\n",
        "    input_tensor = torch.FloatTensor(input_vector)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "    window[f'{predicted.item()}'].update(value=True)\n",
        "    event, values = window.read()\n",
        "    if values['neg']:\n",
        "        label = 0\n",
        "    elif values['pos']:\n",
        "        label = 1\n",
        "    X_labeled = np.vstack((X_labeled, input_vector))\n",
        "    y_labeled.append(label)\n",
        "    y_tensor = torch.LongTensor(y_labeled)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(torch.FloatTensor(X_labeled))\n",
        "        loss = criterion(outputs, y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJaa1ZbMBao3"
      },
      "source": [
        "Note that this is just an example, and you may need to modify the code to suit your specific use case.you may want to modify the code to suit your specific use case, such as changing the layout and design of the PySimpleGUI interface or adjusting the hyperparameters of the model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

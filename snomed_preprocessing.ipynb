{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_csv(\"concepts_terms.txt\")\n",
    "df = pd.read_csv(\"concepts.csv\")\n",
    "text = df[\"term\"]\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_length = list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = []\n",
    "for term in text:\n",
    "    total_text.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neoplasm of anterior aspect of epiglottis\n",
      "Neoplasm of junctional region of epiglottis\n",
      "Neoplasm of lateral wall of oropharynx\n",
      "Neoplasm of posterior wall of oropharynx\n",
      "Neoplasm of esophagus\n"
     ]
    }
   ],
   "source": [
    "for t in total_text[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_csv(\"concepts_terms.txt\")\n",
    "df = pd.read_csv(\"descriptions.csv\")\n",
    "description = df[\"term\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_desc = []\n",
    "for desc in description:\n",
    "    total_desc.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A decrease in lower leg circumference due to recurrent ulceration and fat necrosis causing loss of subcutaneous tissue in a patient with venous stasis disease\n",
      "Introduction of a substance to the body\n",
      "Domestic goat\n",
      "Domestic sheep species\n",
      "An implantation of a staple\n"
     ]
    }
   ],
   "source": [
    "for t in total_desc[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_desc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_desc= dict(zip(total_text, total_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_desc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_desc.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[term_desc.update({term:desc}) for term in total_text for desc in total_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'concepts.text') as fileObject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code forconverting text file to csv\n",
    "#import pandas as pd\n",
    "\n",
    "#read_file = pd.read_csv (r'E:\\UPM\\ADEs\\SnomedCT_USEditionRF2_PRODUCTION_20220301T120000Z\\Terms and definitions\\concepts_descriptions.txt', sep=\"\\t\")\n",
    "#read_file.to_csv (r'E:\\UPM\\ADEs\\SnomedCT_USEditionRF2_PRODUCTION_20220301T120000Z\\Terms and definitions\\descriptions.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x000001C7A6F215B0>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "length = map(len, map(nltk.word_tokenize, total_text))\n",
    "\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568362"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025765\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for i in range(900000):\n",
    "    count += len(total_text[i].split())\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [\"sokoto\", \"nigeria\", \"sokoto state university\", 0.2, 3.0, \"kano\", \"Enugu state nigeria state university\", 2.33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7890103\n"
     ]
    }
   ],
   "source": [
    "count1 = 0\n",
    "t_test = \" \"\n",
    "for x in range(len(total_text)):\n",
    "    if type(total_text[x])==float:\n",
    "        continue\n",
    "    else:\n",
    "        count1 += len(total_text[x].split())\n",
    "        t_test += total_text[x] + \" \"\n",
    "        #print(text1[x])\n",
    "\n",
    "print(count1)\n",
    "#print(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      " sokoto nigeria sokoto state university kano Enugu state nigeria state university \n"
     ]
    }
   ],
   "source": [
    "count2 = 0\n",
    "t_text = \" \"\n",
    "for x in range(len(text1)):\n",
    "    if type(text1[x])==float:\n",
    "        continue\n",
    "    else:\n",
    "        count2 += len(text1[x].split())\n",
    "        t_text += text1[x] + \" \"\n",
    "        \n",
    "\n",
    "print(count2)\n",
    "print(t_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neoplasm of anterior aspect of epiglottis Neoplas'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test[1:50:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def collect_total_dataset(file_path):\n",
    "    total_dataset = \"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    text = df[\"term\"]\n",
    "    total_word_count = 0\n",
    "    total_text = []\n",
    "    for term in text:\n",
    "        total_text.append(term)\n",
    "    \n",
    "    for x in range(len(total_text)):\n",
    "        if type(total_text[x])==float:\n",
    "            continue\n",
    "        else:\n",
    "            total_word_count += len(total_text[x].split())\n",
    "            total_dataset += total_text[x] + \" \"\n",
    "            \n",
    "\n",
    "    return total_dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = collect_total_dataset(\"concepts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neoplasm of anterior'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0:20:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import pandas as pd\n",
    "def collect_total_dataset(file_path):\n",
    "   \n",
    "\n",
    "#df = pd.read_csv(\"concepts_terms.txt\")\n",
    "df = pd.read_csv(\"concepts.csv\")\n",
    "text = df[\"term\"]\n",
    "#text\n",
    "total_text = []\n",
    "for term in text:\n",
    "    total_text.append(term)\n",
    "t_test = \" \"\n",
    "for x in range(len(total_text)):\n",
    "    if type(total_text[x])==float:\n",
    "        continue\n",
    "    else:\n",
    "        count1 += len(total_text[x].split())\n",
    "        t_test += total_text[x] + \" \"\n",
    "        #print(text1[x])\n",
    "\n",
    "print(count1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verify PaddlePaddle program ... \n",
      "PaddlePaddle works well on 1 CPU.\n",
      "PaddlePaddle works well on 2 CPUs.\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "paddle.utils.run_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5588\\896720876.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mchar_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mroughly_word_size\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtotal_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m    \u001b[1;31m# char_size += len(text1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(\"the number of charcters in the dataset is: \", char_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "roughly_word_size = []\n",
    "char_size = 0\n",
    "for sample in range(len(total_text)):\n",
    "    roughly_word_size += total_text[sample]\n",
    "   # char_size += len(text1)\n",
    "#print(\"the number of charcters in the dataset is: \", char_size)\n",
    "print(\"the number of words in the dataset is: \", roughly_word_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting CoLA...\n",
      "\tCompleted!\n",
      "Downloading and extracting SST...\n",
      "\tCompleted!\n",
      "Processing MRPC...\n",
      "\tCompleted!\n",
      "Downloading and extracting QQP...\n",
      "\tCompleted!\n",
      "Downloading and extracting STS...\n",
      "\tCompleted!\n",
      "Downloading and extracting MNLI...\n",
      "\tNote (12/10/20): This script no longer downloads SNLI. You will need to manually download and format the data to use SNLI.\n",
      "\tCompleted!\n",
      "Downloading and extracting QNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting RTE...\n",
      "\tCompleted!\n",
      "Downloading and extracting WNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting diagnostic...\n",
      "\tCompleted!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "''' Script for downloading all GLUE data.\n",
    "Note: for legal reasons, we are unable to host MRPC.\n",
    "You can either use the version hosted by the SentEval team, which is already tokenized, \n",
    "or you can download the original data from (https://download.microsoft.com/download/D/4/6/D46FF87A-F6B9-4252-AA8B-3604ED519838/MSRParaphraseCorpus.msi) and extract the data from it manually.\n",
    "For Windows users, you can run the .msi file. For Mac and Linux users, consider an external library such as 'cabextract' (see below for an example).\n",
    "You should then rename and place specific files in a folder (see below for an example).\n",
    "mkdir MRPC\n",
    "cabextract MSRParaphraseCorpus.msi -d MRPC\n",
    "cat MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D | tr -d $'\\r' > MRPC/msr_paraphrase_train.txt\n",
    "cat MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61 | tr -d $'\\r' > MRPC/msr_paraphrase_test.txt\n",
    "rm MRPC/_*\n",
    "rm MSRParaphraseCorpus.msi\n",
    "1/30/19: It looks like SentEval is no longer hosting their extracted and tokenized MRPC data, so you'll need to download the data from the original source for now.\n",
    "2/11/19: It looks like SentEval actually *is* hosting the extracted data. Hooray!\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import tempfile\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import io\n",
    "URLLIB = urllib.request\n",
    "\n",
    "TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"QNLI\", \"RTE\", \"WNLI\", \"diagnostic\"]\n",
    "TASK2PATH = {\"CoLA\":'https://dl.fbaipublicfiles.com/glue/data/CoLA.zip',\n",
    "             \"SST\":'https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',\n",
    "             \"QQP\":'https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip',\n",
    "             \"STS\":'https://dl.fbaipublicfiles.com/glue/data/STS-B.zip',\n",
    "             \"MNLI\":'https://dl.fbaipublicfiles.com/glue/data/MNLI.zip',\n",
    "             \"QNLI\":'https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip',\n",
    "             \"RTE\":'https://dl.fbaipublicfiles.com/glue/data/RTE.zip',\n",
    "             \"WNLI\":'https://dl.fbaipublicfiles.com/glue/data/WNLI.zip',\n",
    "             \"diagnostic\":'https://dl.fbaipublicfiles.com/glue/data/AX.tsv',\n",
    "             \"MRPC\":'https://raw.githubusercontent.com/MegEngine/Models/master/official/nlp/bert/glue_data/MRPC/dev_ids.tsv'}\n",
    "\n",
    "#MRPC_TRAIN = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt'\n",
    "#MRPC_TEST = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt'\n",
    "\n",
    "def download_and_extract(task, data_dir):\n",
    "    print(\"Downloading and extracting %s...\" % task)\n",
    "    if task == \"MNLI\":\n",
    "        print(\"\\tNote (12/10/20): This script no longer downloads SNLI. You will need to manually download and format the data to use SNLI.\")\n",
    "    data_file = \"%s.zip\" % task\n",
    "    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n",
    "    with zipfile.ZipFile(data_file) as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(data_file)\n",
    "    print(\"\\tCompleted!\")\n",
    "\n",
    "def format_mrpc(data_dir, path_to_data):\n",
    "    print(\"Processing MRPC...\")\n",
    "    mrpc_dir = os.path.join(data_dir, \"MRPC\")\n",
    "    if not os.path.isdir(mrpc_dir):\n",
    "        os.mkdir(mrpc_dir)\n",
    "    if path_to_data:\n",
    "        mrpc_train_file = os.path.join(path_to_data, \"msr_paraphrase_train.txt\")\n",
    "        mrpc_test_file = os.path.join(path_to_data, \"msr_paraphrase_test.txt\")\n",
    "    else:\n",
    "        try:\n",
    "            mrpc_train_file = os.path.join(mrpc_dir, \"msr_paraphrase_train.txt\")\n",
    "            mrpc_test_file = os.path.join(mrpc_dir, \"msr_paraphrase_test.txt\")\n",
    "            URLLIB.urlretrieve(MRPC_TRAIN, mrpc_train_file)\n",
    "            URLLIB.urlretrieve(MRPC_TEST, mrpc_test_file)\n",
    "        except urllib.error.HTTPError:\n",
    "            print(\"Error downloading MRPC\")\n",
    "            return\n",
    "    assert os.path.isfile(mrpc_train_file), \"Train data not found at %s\" % mrpc_train_file\n",
    "    assert os.path.isfile(mrpc_test_file), \"Test data not found at %s\" % mrpc_test_file\n",
    "\n",
    "    with io.open(mrpc_test_file, encoding='utf-8') as data_fh, \\\n",
    "            io.open(os.path.join(mrpc_dir, \"test.tsv\"), 'w', encoding='utf-8') as test_fh:\n",
    "        header = data_fh.readline()\n",
    "        test_fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
    "        for idx, row in enumerate(data_fh):\n",
    "            label, id1, id2, s1, s2 = row.strip().split('\\t')\n",
    "            test_fh.write(\"%d\\t%s\\t%s\\t%s\\t%s\\n\" % (idx, id1, id2, s1, s2))\n",
    "\n",
    "    try:\n",
    "        URLLIB.urlretrieve(TASK2PATH[\"MRPC\"], os.path.join(mrpc_dir, \"dev_ids.tsv\"))\n",
    "    except KeyError or urllib.error.HTTPError:\n",
    "        print(\"\\tError downloading standard development IDs for MRPC. You will need to manually split your data.\")\n",
    "        return\n",
    "\n",
    "    dev_ids = []\n",
    "    with io.open(os.path.join(mrpc_dir, \"dev_ids.tsv\"), encoding='utf-8') as ids_fh:\n",
    "        for row in ids_fh:\n",
    "            dev_ids.append(row.strip().split('\\t'))\n",
    "\n",
    "    with io.open(mrpc_train_file, encoding='utf-8') as data_fh, \\\n",
    "         io.open(os.path.join(mrpc_dir, \"train.tsv\"), 'w', encoding='utf-8') as train_fh, \\\n",
    "         io.open(os.path.join(mrpc_dir, \"dev.tsv\"), 'w', encoding='utf-8') as dev_fh:\n",
    "        header = data_fh.readline()\n",
    "        train_fh.write(header)\n",
    "        dev_fh.write(header)\n",
    "        for row in data_fh:\n",
    "            label, id1, id2, s1, s2 = row.strip().split('\\t')\n",
    "            if [id1, id2] in dev_ids:\n",
    "                dev_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "            else:\n",
    "                train_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "                \n",
    "    print(\"\\tCompleted!\")\n",
    "    \n",
    "def download_diagnostic(data_dir):\n",
    "    print(\"Downloading and extracting diagnostic...\")\n",
    "    if not os.path.isdir(os.path.join(data_dir, \"diagnostic\")):\n",
    "        os.mkdir(os.path.join(data_dir, \"diagnostic\"))\n",
    "    data_file = os.path.join(data_dir, \"diagnostic\", \"diagnostic.tsv\")\n",
    "    urllib.request.urlretrieve(TASK2PATH[\"diagnostic\"], data_file)\n",
    "    print(\"\\tCompleted!\")\n",
    "    return\n",
    "\n",
    "def get_tasks(task_names):\n",
    "    task_names = task_names.split(',')\n",
    "    if \"all\" in task_names:\n",
    "        tasks = TASKS\n",
    "    else:\n",
    "        tasks = []\n",
    "        for task_name in task_names:\n",
    "            assert task_name in TASKS, \"Task %s not found!\" % task_name\n",
    "            tasks.append(task_name)\n",
    "    return tasks\n",
    "\n",
    "def main(arguments):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', help='directory to save data to', type=str, default='glue_data')\n",
    "    parser.add_argument('--tasks', help='tasks to download data for as a comma separated string',\n",
    "                        type=str, default='all')\n",
    "    parser.add_argument('--path_to_mrpc', help='path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt',\n",
    "                        type=str, default='')\n",
    "    #args = parser.parse_args(arguments)\n",
    "    args = parser.parse_args(args=[])\n",
    "    #args = parser.parse_args(sys.argv[1:])\n",
    "\n",
    "    if not os.path.isdir(args.data_dir):\n",
    "        os.mkdir(args.data_dir)\n",
    "    tasks = get_tasks(args.tasks)\n",
    "\n",
    "    for task in tasks:\n",
    "        if task == 'MRPC':\n",
    "            format_mrpc(args.data_dir, args.path_to_mrpc)\n",
    "        elif task == 'diagnostic':\n",
    "            download_diagnostic(args.data_dir)\n",
    "        else:\n",
    "            download_and_extract(task, args.data_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main(sys.argv[1:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2572738177.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5588\\2572738177.py\"\u001b[1;36m, line \u001b[1;32m19\u001b[0m\n\u001b[1;33m    cat $INPUT/CoLA/train.tsv | awk -F\"\\t\"  '{if(NR==1){print \"label\\ttext_a\"} else {print $2\"\\t\"$4}}' > ./glue_data_processed/CoLA/train.tsv\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/bin/bash\n",
    "set -ex\n",
    "#R_DIR=`glue_data $0`; MY_DIR=`cd $R_DIR;`; \n",
    "R_DIR =\"glue_data\"; MY_DIR= 'cd /d \"%~dp0\"';\n",
    "\n",
    "#INPUT=$1\n",
    "import os\n",
    "import sys\n",
    "sys.argv[1]\n",
    "\n",
    "#if [[ ! -d ./glue_data_processed/ ]];then\n",
    "#    mkdir ./glue_data_processed/\n",
    "#fi\n",
    "if not os.is_path('./glue_data_processed/'):\n",
    "    mkdir (\"./glue_data_processes/\")\n",
    "\n",
    "### CoLA\n",
    "mkdir (\"./glue_data_processed/CoLA\")\n",
    "cat $INPUT/CoLA/train.tsv | awk -F\"\\t\"  '{if(NR==1){print \"label\\ttext_a\"} else {print $2\"\\t\"$4}}' > ./glue_data_processed/CoLA/train.tsv\n",
    "cat $INPUT/CoLA/dev.tsv   | awk -F\"\\t\"  '{if(NR==1){print \"label\\ttext_a\"} else {print $2\"\\t\"$4}}' > ./glue_data_processed/CoLA/dev.tsv\n",
    "cat $INPUT/CoLA/test.tsv  | awk -F\"\\t\"  '{if(NR==1){print \"qid\\ttext_a\\tlabel\"}   else {print $0\"\\t-1\"}}'       > ./glue_data_processed/CoLA/test.tsv\n",
    "\n",
    "### SST-2\n",
    "mkdir -p ./glue_data_processed/SST-2\n",
    "cat $INPUT/SST-2/train.tsv | awk -F\"\\t\"    '{if(NR==1){print \"label\\ttext_a\"}  else if(NF==2) {print $2\"\\t\"$1}}' > ./glue_data_processed/SST-2/train.tsv\n",
    "cat $INPUT/SST-2/dev.tsv   | awk -F\"\\t\"    '{if(NR==1){print \"label\\ttext_a\"}  else if(NF==2) {print $2\"\\t\"$1}}' > ./glue_data_processed/SST-2/dev.tsv\n",
    "cat $INPUT/SST-2/test.tsv  | awk -F\"\\t\"    '{if(NR==1){print \"qid\\ttext_a\\tlabel\"}    else {print $0\"\\t-1\"}}'    > ./glue_data_processed/SST-2/test.tsv\n",
    "\n",
    "### MRPC\n",
    "mkdir -p ./glue_data_processed/MRPC\n",
    "cat $INPUT/MRPC/train.tsv | awk -F\"\\t\" '{if(NR==1){print \"text_a\\ttext_b\\tlabel\"} else{print $4\"\\t\"$5\"\\t\"$1}}' > ./glue_data_processed/MRPC/train.tsv\n",
    "cat $INPUT/MRPC/dev.tsv   | awk -F\"\\t\" '{if(NR==1){print \"text_a\\ttext_b\\tlabel\"} else{print $4\"\\t\"$5\"\\t\"$1}}' > ./glue_data_processed/MRPC/dev.tsv\n",
    "cat $INPUT/MRPC/test.tsv  | awk -F\"\\t\" '{if(NR==1){print \"qid\\ttext_a\\ttext_b\\tlabel\"}   else{print $1\"\\t\"$4\"\\t\"$5\"\\t-1\"}}' > ./glue_data_processed/MRPC/test.tsv\n",
    "\n",
    "### STS-B\n",
    "mkdir -p ./glue_data_processed/STS-B\n",
    "cat $INPUT/STS-B/train.tsv | awk -F\"\\t\" '{if(NR==1){print \"text_a\\ttext_b\\tlabel\"} else{print $8\"\\t\"$9\"\\t\"$10}}' > ./glue_data_processed/STS-B/train.tsv\n",
    "cat $INPUT/STS-B/dev.tsv   | awk -F\"\\t\" '{if(NR==1){print \"text_a\\ttext_b\\tlabel\"} else{print $8\"\\t\"$9\"\\t\"$10}}' > ./glue_data_processed/STS-B/dev.tsv\n",
    "cat $INPUT/STS-B/test.tsv  | awk -F\"\\t\" '{if(NR==1){print \"qid\\ttext_a\\ttext_b\\tlabel\"}   else{print $1\"\\t\"$8\"\\t\"$9\"\\t-1\"}}'  > ./glue_data_processed/STS-B/test.tsv\n",
    "\n",
    "### QQP\n",
    "mkdir -p ./glue_data_processed/QQP\n",
    "cat $INPUT/QQP/train.tsv | awk -F\"\\t\"  '{if(NR==1){print \"text_a\\ttext_b\\tlabel\"} else if($6!=\"\") {print $4\"\\t\"$5\"\\t\"$6}}' > ./glue_data_processed/QQP/train.tsv\n",
    "cat $INPUT/QQP/dev.tsv   | awk -F\"\\t\"  '{if(NR==1){print \"text_a\\ttext_b\\tlabel\"} else if($6!=\"\") {print $4\"\\t\"$5\"\\t\"$6}}' > ./glue_data_processed/QQP/dev.tsv\n",
    "cat $INPUT/QQP/test.tsv  | awk -F\"\\t\"  '{if(NR==1){print \"qid\\ttext_a\\ttext_b\\tlabel\"}   else {print $0\"\\t-1\"}}'           > ./glue_data_processed/QQP/test.tsv\n",
    "\n",
    "### MNLI\n",
    "mkdir -p ./glue_data_processed/MNLI\n",
    "cat $INPUT/MNLI/train.tsv            | python $MY_DIR/mnli.py > ./glue_data_processed/MNLI/train.tsv\n",
    "\n",
    "mkdir -p ./glue_data_processed/MNLI/m\n",
    "cat $INPUT/MNLI/dev_matched.tsv      | python $MY_DIR/mnli.py > ./glue_data_processed/MNLI/m/dev.tsv\n",
    "cat $INPUT/MNLI/test_matched.tsv     | python $MY_DIR/mnli.py > ./glue_data_processed/MNLI/m/test.tsv\n",
    "\n",
    "mkdir -p ./glue_data_processed/MNLI/mm\n",
    "cat $INPUT/MNLI/dev_mismatched.tsv   | python $MY_DIR/mnli.py  > ./glue_data_processed/MNLI/mm/dev.tsv\n",
    "cat $INPUT/MNLI/test_mismatched.tsv  | python $MY_DIR/mnli.py > ./glue_data_processed/MNLI/mm/test.tsv\n",
    "\n",
    "### QNLI\n",
    "mkdir -p ./glue_data_processed/QNLI\n",
    "cat $INPUT/QNLI/train.tsv | python $MY_DIR/qnli.py > ./glue_data_processed/QNLI/train.tsv\n",
    "cat $INPUT/QNLI/dev.tsv   | python $MY_DIR/qnli.py > ./glue_data_processed/QNLI/dev.tsv\n",
    "cat $INPUT/QNLI/test.tsv  | python $MY_DIR/qnli.py > ./glue_data_processed/QNLI/test.tsv\n",
    "\n",
    "### RTE\n",
    "mkdir -p ./glue_data_processed/RTE\n",
    "cat $INPUT/RTE/train.tsv | python $MY_DIR/qnli.py > ./glue_data_processed/RTE/train.tsv\n",
    "cat $INPUT/RTE/dev.tsv   | python $MY_DIR/qnli.py > ./glue_data_processed/RTE/dev.tsv\n",
    "cat $INPUT/RTE/test.tsv  | python $MY_DIR/qnli.py > ./glue_data_processed/RTE/test.tsv\n",
    "\n",
    "### WNLI\n",
    "mkdir -p ./glue_data_processed/WNLI\n",
    "cat $INPUT/WNLI/train.tsv | awk -F\"\\t\"  '{if(NR==1){print \"text_a\\ttext_b\\tlabel\"} else {print $2\"\\t\"$3\"\\t\"$4}}' > ./glue_data_processed/WNLI/train.tsv\n",
    "cat $INPUT/WNLI/dev.tsv   | awk -F\"\\t\"  '{if(NR==1){print \"text_a\\ttext_b\\tlabel\"} else {print $2\"\\t\"$3\"\\t\"$4}}' > ./glue_data_processed/WNLI/dev.tsv\n",
    "cat $INPUT/WNLI/test.tsv  | awk -F\"\\t\"  '{if(NR==1){print \"qid\\ttext_a\\ttext_b\\tlabel\"}   else {print $1\"\\t\"$2\"\\t\"$3\"\\t-1\"}}' > ./glue_data_processed/WNLI/test.tsv\n",
    "\n",
    "### Diagnostics\n",
    "cat $INPUT/diagnostic/diagnostic.tsv | awk -F\"\\t\"  '{if(NR==1){print \"qid\\ttext_a\\ttext_b\\tlabel\"} else {print $0\"\\t-1\"}}'         > ./glue_data_processed/MNLI/diagnostic.tsv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING TRANSFORMER MODEL FROM EXISTING TRANSFORMER ROBARTA\n",
    "pip install datasets\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"amazon_reviews_multi\" \"en\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roughly_word_size = 0\n",
    "char_size = 0\n",
    "for sample in dataset[\"train\"][\"review_body\"]\n",
    "    roughly_word_size += len(sample.split())\n",
    "    char_size += len(sample)\n",
    "print(char_size)\n",
    "print(roughly_word_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tokenizers\n",
    "from tokenizer import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2000\n",
    "tokenizer.train_from_iterator(dataset[\"train\"][\"review_body\"], special_token = \"\", \"\", \"\", vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode_batch(dataset[\"train\"][\"review_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(tokenized_text[0].ids)\n",
    "np.array(tokenized_text[0].tokens)\n",
    "\n",
    "tokenizer.decode(tokenized_text[0].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers\n",
    "from tokenizers.implementation import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(./tokenizer/vocab.json, ./tokenizer/merges.txt,)\n",
    "tokenizer._tokenizer.post_processor = BertProcessing((\" \", tokenizer.token_to_id(\" \")),(\" \", tokenizer.token_to_id(\" \")),)\n",
    "tokenizer.enable_truncation(max_lenth = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobartaConfig\n",
    "\n",
    "config = RobartaConfig(vocab_size = vocab_size+5,\n",
    "                    max_position_embeddings = 514,\n",
    "                    num_attention_heads = 12,\n",
    "                    num_hidden_layers = 6,\n",
    "                    type_vocab_size = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobartaTokenizerFast\n",
    "tokenizer = RobartaTokenizerFast.from_pretrained(\"./tokenizer\", max_length = 512, truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobartaForMaskedLM\n",
    "\n",
    "model = RobartaForMaskedLM(config = config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"train\"][\"review_body\"], truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched = True, num_proc =4, remove_columns=[\"review_body\"])\n",
    "block_size = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    #concatenate all text\n",
    "    concatenated_examples = {k:num(examples[k],[]) for k in examples.keys()}\n",
    "    # we drop the remainder of the tokens or customize to meet you aim in your work\n",
    "    total_length = (total_length//block_size)*block_size\n",
    "    # split by chunk of max_length\n",
    "\n",
    "    result = {k: (t[i:i + block_size] for i in range (0, total_length, block_size))for k,t in concatenated_examples.item()}\n",
    "\n",
    "    result [\"labels\"] = result [\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\"review_id\", \"product_id\", \"stars\", \"review_title\", \"language\", \"product_category\")\n",
    "im_datasets = tokenized_datasets.map(group_texts, batched = True, batch_size = 1000, num_proc = 4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_agrs = TrainingArguments(f\"amazon_reviews\", evaluation_strategy = \"epoch\", learning_rate = 2e-5, weight_decay = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollectorForLanguageModeling \n",
    "datacollector = DataCollectorForLanguageModeling(tokenizer = tokezer, mlm=True, mlm_probability = 0.15)\n",
    "trainer = Trainer (model = model,\n",
    "                    args = training_args,\n",
    "                    train_dataset = im_datasets[\"train\"], eval_dataset= im_datasets[\"validation\"], \n",
    "                    data_collector = datacollector,\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
